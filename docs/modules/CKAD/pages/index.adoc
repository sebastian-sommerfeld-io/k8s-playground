= Certified Kubernetes Application Developer

The Certified Kubernetes Application Developer (CKAD) program has been developed by the Cloud Native Computing Foundation (CNCF), in collaboration with The Linux Foundation, to help expand the Kubernetes ecosystem through standardized training and certification.

*Who Is It For:* This certification is for Kubernetes engineers, cloud engineers and other IT professionals responsible for building, deploying, and configuring cloud native applications with Kubernetes.

*About This Certification:* CKAD has been developed by The Linux Foundation and the Cloud Native Computing Foundation (CNCF), to help expand the Kubernetes ecosystem through standardized training and certification. This exam is an online, proctored, performance-based test that consists of a set of performance-based tasks (problems) to be solved in a command line.

*What It Demonstrates:* The Certified Kubernetes Application Developer (CKAD) can design, build and deploy cloud-native applications for Kubernetes. A CKAD can define application resources and use Kubernetes core primitives to create/migrate, configure, expose and observe scalable applications.
The exam assumes working knowledge of container runtimes and microservice architecture. The successful candidate will be comfortable (1) working with (OCI-compliant) container images, (2) applying Cloud Native application concepts and architectures and (3) working with and validating Kubernetes resource definitions.

* Udemy Course: https://www.udemy.com/course/certified-kubernetes-application-developer/learn/lecture/12299352#overview
* Cert: https://training.linuxfoundation.org/certification/certified-kubernetes-application-developer-ckad/ (about the cert: https://www.cncf.io/certification/ckad)


== Udemy Section 1: Course Content and Introduction
The CKAD test to obtain the cert is a hands-on practical exam. So it is important to practice what you learn. So make use of the browser-lab which accompanies the Udemy course.


== Udemy Section 2: Core Concepts
=== Kubernetes Architecture
. Node = Machine (physical or virtual) = Worker Node = The place where containers run
. Cluster = Set of nodes
. Master = Node which is configured as a Master -> responsible for the orchestration of containers on the actual worker nodes

When installing kubernetes, the following *components* are installed

. API Server -> Frontend for Kubernetes (Users, CLI, management devices all talk to the API server to interact with the cluser)
. `etcd` (Service) -> Distributed key-value store (stores information from all nodes and all worker nodes in a distributed way)
. `kubelet` (Service) -> The agent running on each node on the cluster and makes sure the containers are running on the nodes as expected.
. Container Runtime -> Underlying softwar to run containers (e.g. Docker)
. Controller -> The brain behind orchestration. Responsible for noticing and responding when nodes, containers or endpoints go down. Controllers make decisions on bringing up new containers.
. Scheduler -> Distributing work / containers accross multiple nodes (looks for newly created containers and assigns them to nodes)

==== How does one server become a master and another one a worker?
Worker nodes come with a container runtime. Master nodes are not intended to run containers. They only control the cluster.

All information gathered from the `kube-apiserver` and the `kubelet` instances are stored in `etcd` on the master.

image:CKAD:02/master-worker.png[]

==== `kubectl`
Kubernetes provides a command line tool for communicating with a Kubernetes cluster's control plane, using the Kubernetes API. This tool is named `kubectl`.

Used to deploy and manage application on akubernetes cluster, to get information on the cluster and cluster nodes and many other things.

==== Master Node vs Control Plane
*Is Kubernetes control plane the same as the master node?* -> A Kubernetes cluster consists of a set of nodes, which all run containerized applications. Of those, a small number are running applications that manage the cluster. They are referred to as master nodes, also collectively known as the control plane.

*From the link:https://kubernetes.io/docs/reference/glossary/?all=true#term-control-plane[Kubernetes Glossary]:* The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.

=== Docker vs ContainerD
image:CKAD:02/ctr-nerdctl-crictl.png[]

* `ctr` is a command-line client shipped as part of the containerd project. If you have containerd running on a machine, chances are the `ctr` binary is also present there. The `ctr` client is similar to Docker's eponymous CLI, but the commands and flags often differ from their (typically more user-friendly) docker analogs. Historically, the primary audience of the `ctr` client was containerd developers themselves testing the daemon. However, `ctr` can serve as a great exploration means - it operates right on top of the containerd API, and by examining the available commands, you can get a rough idea of what containerd can or cannot do.
* link:https://github.com/containerd/nerdctl[`nerdctl`] is a Docker-compatible CLI for link:https://containerd.io[containerd].
* `crictl` is a command-line interface for CRI-compatible container runtimes. You can use it to inspect and debug container runtimes and applications on a Kubernetes node. 

==== Mandatory yaml fields
A Kubernetes yaml file always needs these 4 parts:
[source, yaml]
----
apiVersion: ...
kind: ...
metadata: ...
spec: ...
----

=== Pods
Kubernetes does not deploy containers directly on the worker node. Containers are encapsulated in a Kubernetes objecet called pod. A pod is a single instance of an application. A pod is the smalles object that you can create in Kubernetes.

*Pods* usually have a 1-to-1 relationship with containers running the application. Scaling takes place through adding/removing pods.

image:CKAD:02/pods-1.png[]

*Multi-Container Pods* are possible but usually the containers inside the pod are not multiple containers of the same kind (= not the same image). Both containers share the lifecycle because they reside in the same Pod. They can communicate to each other via `localhost` since they share the same network space. They can also share the same storage space as well. But multi-container pods are a rare usecase.

image:CKAD:02/pods-2.png[]

==== A Note on Editing Existing Pods
In any of the practical quizzes if you are asked to edit an existing pod, please note the following:

* If you are given a pod definition file, edit that file and use it to create a new pod.
* If you are not given a pod definition file, you may extract the definition to a file using the below command: `kubectl get pod <pod-name> -o yaml > pod-definition.yaml`, Then edit the file to make the necessary changes, delete and re-create the pod.
* Use the `kubectl edit pod <pod-name>` command to edit pod properties.

=== Replication Controller + Replica Sets
The Replication Controller is the older technology which is now replaced by Replica Set.

=== Deployments
Manages deployment strategies (rolling blue/green / canary) and rollbacks.

A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a corolled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.

image:CKAD:02/deployment.png[]

The following are typical use cases for Deployments:

* Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.
* Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.
* Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.
* Scale up the Deployment to facilitate more load.
* Pause the rollout of a Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.
* Use the status of the Deployment as an indicator that a rollout has stuck.
* Clean up older ReplicaSets that you don't need anymore.

=== Namespaces
In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc).

Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.

Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.

Kubernetes starts with four initial namespaces:

* `default` -> Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.
* `kube-node-lease` -> This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.
* `kube-public` -> This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.
* `kube-system` -> The namespace for objects created by the Kubernetes system.

When accessing services within the same namespace, using the service name is enough. Accessing services from other namespaces requires using a fully quallified DNS name (which is created automatically).

image:CKAD:02/namespace-dns.png[]

=== Resource Quota
When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources. Resource quotas are a tool for administrators to address this concern.

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.

Resource quotas work like this:

* Different teams work in different namespaces. This can be enforced with RBAC.
* The administrator creates one ResourceQuota for each namespace.
* Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.
* If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.
* If quota is enabled in a namespace for compute resources like cpu and memory, users must specify requests or limits for those values; otherwise, the quota system may reject pod creation. 

== Udemy Section 3: Configuration
Remember, you cannot edit specifications of an existingpod  other than the below.

* `spec.containers[*].image`
* `spec.initContainers[*].image`
* `spec.activeDeadlineSeconds`
* `spec.tolerations`

For example you cannot edit the environment variables, service accounts, resource limits of a running pod. You have to delete the pod and create a new one.

With Deployments you can easily edit any field/property of the ppd template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a pod part of a deployment you may do that simply by running the command `kubectl edit deployment my-deployment `.`

=== ConfigMaps
A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.

Use `kubectl create configmap` to create a ConfigMap. To initialize the yaml file, use `kubectl create configmap --dry-run=client -o yaml > some-configmap.yml`

A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.

For example, imagine that you are developing an application that you can run on your own computer (for development) and in the cloud (to handle real traffic). You write the code to look in an environment variable named DATABASE_HOST. Locally, you set that variable to localhost. In the cloud, you set it to refer to a Kubernetes Service that exposes the database component to your cluster. This lets you fetch a container image running in the cloud and debug the exact same code locally if needed.

A ConfigMap is not designed to hold large chunks of data. The data stored in a ConfigMap cannot exceed 1 MiB. If you need to store settings that are larger than this limit, you may want to consider mounting a volume or use a separate database or file service.

== Udemy Section 4: Multi-Container Pods

== Udemy Section 5: Observability

== Udemy Section 6: Pod Design

== Udemy Section 7: Services & Networking

== Udemy Section 8: State Persistence

== Udemy Section 9: Updates for Sept. 2021 Changes

== Udemy Section 10: Additional Practice - Kubernetes Challenges

== Udemy Section 11: Certification Tips
=== Imperative Commands
While you would be working mostly the declarative way - using definition files, imperative commands can help in getting one-time tasks done quickly, as well as generate a definition template easily. This would help save a considerable amount of time during your exams.

Familiarize yourself with the two options that can come in handy while working with the below commands:

. `--dry-run`: By default, as soon as the command is run, the resource will be created. If you simply want to test your command, use the `--dry-run=client` option. This will not create the resource. Instead, tell you whether the resource can be created and if your command is right.
. `-o yaml`: This will output the resource definition in YAML format on the screen.

Use the above two in combination along with Linux output redirection to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.

`kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx-pod.yaml`

==== Pod
* Create an NGINX Pod: `kubectl run nginx --image=nginx`
* Generate POD Manifest YAML file (-o yaml), don't create it(--dry-run): `kubectl run nginx --image=nginx --dry-run=client -o yaml`

==== Deployment
* Create a deployment: `kubectl create deployment --image=nginx nginx`
* Generate Deployment YAML file (-o yaml). Don't create it(--dry-run): `kubectl create deployment --image=nginx nginx --dry-run -o yaml`
* Generate Deployment with 4 Replicas: `kubectl create deployment nginx --image=nginx --replicas=4`
* You can also scale deployment using the kubectl scale command: `kubectl scale deployment nginx --replicas=4`
* Another way to do this is to save the YAML definition to a file and modify: `kubectl create deployment nginx --image=nginx--dry-run=client -o yaml > nginx-deployment.yaml` -> You can then update the YAML file with the replicas or any other field before creating the deployment.

==== Service
* Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
** `kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml` -> This will automatically use the pod's labels as selectors.
** `kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml` -> This will not use the pods' labels as selectors; instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work well if your pod has a different label set. So generate the file and modify the selectors before creating the service.
* Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes
** `kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml` -> This will automatically use the pod's labels as selectors, but you cannot specify the node port. You have to generate a definition file and then add the node port in manually before creating the service with the pod.
** `kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml` -> This will not use the pods' labels as selectors.

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the `kubectl expose` command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

== Udemy Section 12: Lightning Labels

== Udemy Section 13: Mock Exams

== Linux Foundation: Exam Preparation
[IMPORTANT]
====
. Where can I find practice questions for CKA/CKAD/CKS? -> https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad-cks#is-there-training-to-prepare-for-the-certification-exam
. Is there training to prepare for the certification exam? -> https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad-cks#is-there-training-to-prepare-for-the-certification-exam-1
. Take a look at the candidate handbook on the vertification website (and the other pages as well) -> https://docs.linuxfoundation.org/tc-docs/certification/lf-handbook2
. Make sure the minimum system requirements are met -> https://syscheck.bridge.psiexams.com
. Read the instructions aboud the CKA and CKAD -> https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad
====
