= Certified Kubernetes Application Developer

The Certified Kubernetes Application Developer (CKAD) program has been developed by the Cloud Native Computing Foundation (CNCF), in collaboration with The Linux Foundation, to help expand the Kubernetes ecosystem through standardized training and certification.

*Who Is It For:* This certification is for Kubernetes engineers, cloud engineers and other IT professionals responsible for building, deploying, and configuring cloud native applications with Kubernetes.

*About This Certification:* CKAD has been developed by The Linux Foundation and the Cloud Native Computing Foundation (CNCF), to help expand the Kubernetes ecosystem through standardized training and certification. This exam is an online, proctored, performance-based test that consists of a set of performance-based tasks (problems) to be solved in a command line.

*What It Demonstrates:* The Certified Kubernetes Application Developer (CKAD) can design, build and deploy cloud-native applications for Kubernetes. A CKAD can define application resources and use Kubernetes core primitives to create/migrate, configure, expose and observe scalable applications.
The exam assumes working knowledge of container runtimes and microservice architecture. The successful candidate will be comfortable (1) working with (OCI-compliant) container images, (2) applying Cloud Native application concepts and architectures and (3) working with and validating Kubernetes resource definitions.

* Udemy Course: https://www.udemy.com/course/certified-kubernetes-application-developer/learn/lecture/12299352#overview
* Cert: https://training.linuxfoundation.org/certification/certified-kubernetes-application-developer-ckad/ (about the cert: https://www.cncf.io/certification/ckad)


== Udemy Section 1: Course Content and Introduction
The CKAD test to obtain the cert is a hands-on practical exam. So it is important to practice what you learn. So make use of the browser-lab which accompanies the Udemy course.


== Udemy Section 2: Core Concepts
=== Kubernetes Architecture
. Node = Machine (physical or virtual) = Worker Node = The place where containers run
. Cluster = Set of nodes
. Master = Node which is configured as a Master -> responsible for the orchestration of containers on the actual worker nodes

When installing kubernetes, the following *components* are installed

. API Server -> Frontend for Kubernetes (Users, CLI, management devices all talk to the API server to interact with the cluser)
. `etcd` (Service) -> Distributed key-value store (stores information from all nodes and all worker nodes in a distributed way)
. `kubelet` (Service) -> The agent running on each node on the cluster and makes sure the containers are running on the nodes as expected.
. Container Runtime -> Underlying softwar to run containers (e.g. Docker)
. Controller -> The brain behind orchestration. Responsible for noticing and responding when nodes, containers or endpoints go down. Controllers make decisions on bringing up new containers.
. Scheduler -> Distributing work / containers accross multiple nodes (looks for newly created containers and assigns them to nodes)

==== How does one server become a master and another one a worker?
Worker nodes come with a container runtime. Master nodes are not intended to run containers. They only control the cluster.

All information gathered from the `kube-apiserver` and the `kubelet` instances are stored in `etcd` on the master.

image:CKAD:02/master-worker.png[]

==== `kubectl`
Kubernetes provides a command line tool for communicating with a Kubernetes cluster's control plane, using the Kubernetes API. This tool is named `kubectl`.

Used to deploy and manage application on akubernetes cluster, to get information on the cluster and cluster nodes and many other things.

==== Master Node vs Control Plane
*Is Kubernetes control plane the same as the master node?* -> A Kubernetes cluster consists of a set of nodes, which all run containerized applications. Of those, a small number are running applications that manage the cluster. They are referred to as master nodes, also collectively known as the control plane.

*From the link:https://kubernetes.io/docs/reference/glossary/?all=true#term-control-plane[Kubernetes Glossary]:* The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.

=== Docker vs ContainerD
image:CKAD:02/ctr-nerdctl-crictl.png[]

* `ctr` is a command-line client shipped as part of the containerd project. If you have containerd running on a machine, chances are the `ctr` binary is also present there. The `ctr` client is similar to Docker's eponymous CLI, but the commands and flags often differ from their (typically more user-friendly) docker analogs. Historically, the primary audience of the `ctr` client was containerd developers themselves testing the daemon. However, `ctr` can serve as a great exploration means - it operates right on top of the containerd API, and by examining the available commands, you can get a rough idea of what containerd can or cannot do.
* link:https://github.com/containerd/nerdctl[`nerdctl`] is a Docker-compatible CLI for link:https://containerd.io[containerd].
* `crictl` is a command-line interface for CRI-compatible container runtimes. You can use it to inspect and debug container runtimes and applications on a Kubernetes node. 

==== Mandatory yaml fields
A Kubernetes yaml file always needs these 4 parts:
[source, yaml]
----
apiVersion: ...
kind: ...
metadata: ...
spec: ...
----

=== Pods
Kubernetes does not deploy containers directly on the worker node. Containers are encapsulated in a Kubernetes objecet called pod. A pod is a single instance of an application. A pod is the smalles object that you can create in Kubernetes.

*Pods* usually have a 1-to-1 relationship with containers running the application. Scaling takes place through adding/removing pods.

image:CKAD:02/pods-1.png[]

*Multi-Container Pods* are possible but usually the containers inside the pod are not multiple containers of the same kind (= not the same image). Both containers share the lifecycle because they reside in the same Pod. They can communicate to each other via `localhost` since they share the same network space. They can also share the same storage space as well. But multi-container pods are a rare usecase.

image:CKAD:02/pods-2.png[]

==== A Note on Editing Existing Pods
In any of the practical quizzes if you are asked to edit an existing pod, please note the following:

* If you are given a pod definition file, edit that file and use it to create a new pod.
* If you are not given a pod definition file, you may extract the definition to a file using the below command: `kubectl get pod <pod-name> -o yaml > pod-definition.yaml`, Then edit the file to make the necessary changes, delete and re-create the pod.
* Use the `kubectl edit pod <pod-name>` command to edit pod properties.

=== Replication Controller + Replica Sets
The Replication Controller is the older technology which is now replaced by Replica Set.

=== Deployments
Manages deployment strategies (rolling blue/green / canary) and rollbacks.

A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a corolled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.

image:CKAD:02/deployment.png[]

The following are typical use cases for Deployments:

* Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.
* Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.
* Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.
* Scale up the Deployment to facilitate more load.
* Pause the rollout of a Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.
* Use the status of the Deployment as an indicator that a rollout has stuck.
* Clean up older ReplicaSets that you don't need anymore.

=== Namespaces
In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc).

Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.

Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.

Kubernetes starts with four initial namespaces:

* `default` -> Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.
* `kube-node-lease` -> This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.
* `kube-public` -> This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.
* `kube-system` -> The namespace for objects created by the Kubernetes system.

When accessing resources within the same namespace, using the resource name is enough. Accessing resources from other namespaces requires using a fully quallified DNS name (which is created automatically).

image:CKAD:02/namespace-dns.png[]

=== Resource Quota
When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources. Resource quotas are a tool for administrators to address this concern.

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.

Resource quotas work like this:

* Different teams work in different namespaces. This can be enforced with RBAC.
* The administrator creates one ResourceQuota for each namespace.
* Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.
* If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.
* If quota is enabled in a namespace for compute resources like cpu and memory, users must specify requests or limits for those values; otherwise, the quota system may reject pod creation. 

== Udemy Section 3: Configuration

== Udemy Section 4: Multi-Container Pods

== Udemy Section 5: Observability

== Udemy Section 6: Pod Design

== Udemy Section 7: Services & Networking

== Udemy Section 8: State Persistence

== Udemy Section 9: Updates for Sept. 2021 Changes

== Udemy Section 10: Additional Practice - Kubernetes Challenges

== Udemy Section 11: Certification Tips

== Udemy Section 12: Lightning Labels

== Udemy Section 13: Mock Exams

== Linux Foundation: Exam Preparation
[IMPORTANT]
====
. Where can I find practice questions for CKA/CKAD/CKS? -> https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad-cks#is-there-training-to-prepare-for-the-certification-exam
. Is there training to prepare for the certification exam? -> https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad-cks#is-there-training-to-prepare-for-the-certification-exam-1
. Take a look at the candidate handbook on the vertification website (and the other pages as well) -> https://docs.linuxfoundation.org/tc-docs/certification/lf-handbook2
. Make sure the minimum system requirements are met -> https://syscheck.bridge.psiexams.com
. Read the instructions aboud the CKA and CKAD -> https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad
====
